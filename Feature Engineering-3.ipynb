{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847adbb7-db76-4977-981e-bb4bb638709c",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38223710-5e4d-4f64-9fa6-e1b7e778b2e0",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features to a specific range. It transforms the values of the features to a common scale between a minimum and maximum value, typically between 0 and 1.\n",
    "\n",
    "The formula to perform Min-Max scaling is as follows:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "where \"value\" is the original value of the feature, \"min_value\" is the minimum value of the feature in the dataset, and \"max_value\" is the maximum value of the feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add33c5c-f69e-44ab-bd39-c32e9dd19aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.25\n",
      "0.5\n",
      "0.75\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example dataset\n",
    "data = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "scaled_data = scaler.fit_transform([[x] for x in data])\n",
    "\n",
    "# Print the scaled values\n",
    "for scaled_value in scaled_data:\n",
    "    print(scaled_value[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf4d84-5f22-440f-91ee-aecb915efc2e",
   "metadata": {},
   "source": [
    "In this example, we have a dataset with values ranging from 10 to 50. We create a MinMaxScaler object and fit it to the data. Then, we use the fit_transform method to both fit the scaler and transform the data simultaneously. Finally, we iterate over the scaled values and print them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768ab2e-88b4-43b3-a1dd-6c72edbb4923",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491180f2-5135-481c-9aa5-c7f7f6e92a5f",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d6e2ed-cc56-4e2b-b9b2-e2e3fc8d2641",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "The Unit Vector technique, also known as normalization, is a feature scaling method that scales the values of a feature to have a unit norm or length. It calculates the magnitude or Euclidean norm of the feature vector and then divides each element of the vector by its norm.\n",
    "\n",
    "The formula to perform Unit Vector scaling is as follows:\n",
    "\n",
    "normalized_value = value / ||vector||\n",
    "\n",
    "where \"value\" is the original value of the feature, \"vector\" is the feature vector, and \"||vector||\" represents the Euclidean norm or magnitude of the vector.\n",
    "\n",
    "Compared to Min-Max scaling, which scales the values to a specific range, Unit Vector scaling normalizes the feature vector to have a length of 1. This technique is particularly useful when the direction or angle of the feature vector is important for analysis or modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daec808c-eb7d-462b-ae4f-ad01f504735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4472136  0.89442719]\n",
      "[0.6 0.8]\n",
      "[0.6401844  0.76822128]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Example dataset\n",
    "data = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "# Create a Normalizer object\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "# Fit the normalizer to the data and transform it\n",
    "normalized_data = normalizer.transform(data)\n",
    "\n",
    "# Print the normalized values\n",
    "for normalized_vector in normalized_data:\n",
    "    print(normalized_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7fe27-1fe2-442b-9990-ddf2b4958328",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acec684-dfcf-48b9-919d-91939ae4fe94",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b71d0-7803-4c7b-9535-48d21cc53108",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction. It transforms a dataset with potentially high-dimensional features into a lower-dimensional space while preserving the most important information or variability in the data. It achieves this by identifying the principal components, which are linear combinations of the original features that capture the maximum variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9489bdc-5c10-4763-8ce0-d27cbb73ebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.79422863  0.        ]\n",
      " [-2.59807621  0.        ]\n",
      " [ 2.59807621  0.        ]\n",
      " [ 7.79422863 -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a PCA object with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA model to the data and transform it\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "# Print the reduced data\n",
    "print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09349f-a825-437f-b53c-d5b51c1f1050",
   "metadata": {},
   "source": [
    "the original 3-dimensional data has been transformed into a 2-dimensional space. The reduced data is represented by the principal components, where the second component has zero variance because the original data is aligned along that axis. The first component captures the maximum variability in the data.\n",
    "\n",
    "PCA can be useful for various purposes, such as visualization, feature extraction, or reducing the dimensionality of the data before applying machine learning algorithms that may struggle with high-dimensional data or to remove redundant or less informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68aa9b-06e7-48ba-9454-ab662e3c7fc2",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d8454-0f91-4c58-9047-9330eaa2c9b0",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c6b18-61a3-4b19-83f4-98a6826bd2e5",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "PCA and feature extraction are closely related concepts. PCA can be used as a technique for feature extraction, where it transforms the original features into a new set of derived features (principal components) that capture the most important information or variability in the data.\n",
    "\n",
    "Feature extraction aims to reduce the dimensionality of the data while retaining the most relevant information. It involves creating a smaller set of features that can effectively represent the original data. PCA achieves this by identifying the principal components, which are linear combinations of the original features. These principal components are ranked based on the amount of variance they capture in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf0c3ad-f3ca-41f3-8c53-4509f5a8e689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pca.fit(data)\n",
    "\n",
    "# Get the explained variance ratio of each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(explained_variance_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbaf43-6ff9-49ad-b189-2a56c6f188e8",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83dbde1-120b-428e-9820-73b17987c381",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63ab3e-591a-4b55-8ec7-4d45a2027edf",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "To preprocess the features in the food delivery service dataset (price, rating, delivery time) using Min-Max scaling, you would follow these steps:\n",
    "\n",
    "1. Import the necessary libraries: You would typically import libraries such as scikit-learn in Python to perform Min-Max scaling.\n",
    "\n",
    "2. Load and prepare the dataset: Load the dataset containing the features (price, rating, delivery time) and any other relevant information.\n",
    "\n",
    "3. Separate the feature columns: Extract the columns containing the features you want to scale (price, rating, delivery time) into a separate dataset or array.\n",
    "\n",
    "4. Apply Min-Max scaling: Create an instance of the MinMaxScaler class from the scikit-learn library and fit it to the data. This step calculates the minimum and maximum values of each feature.\n",
    "\n",
    "5. Transform the data: Use the `transform` method of the MinMaxScaler object to scale the features in the dataset using the calculated minimum and maximum values.\n",
    "\n",
    "6. Use the scaled data: The transformed data will now have the scaled values between 0 and 1. You can use this scaled data for building your recommendation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff786eea-6c56-4d5a-9637-12e46b121d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('food_delivery_datasets.csv')\n",
    "\n",
    "# Extract the feature columns\n",
    "features = data[['food_price', 'rating', 'eta_seconds']]\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Create a new dataframe with the scaled features\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['scaled_price', 'scaled_rating', 'scaled_delivery_time'])\n",
    "\n",
    "# Use the scaled data for the recommendation system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786ba1b-4531-4a66-a8ff-297742901886",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this example, the dataset is loaded using `pd.read_csv` from a CSV file (assuming the file is named 'food_delivery_data.csv'). The feature columns (price, rating, delivery_time) are extracted into the `features` DataFrame. A MinMaxScaler object is created using `MinMaxScaler()`, and then the `fit_transform` method is used to fit the scaler to the data and transform it. The transformed features are stored in the `scaled_data` DataFrame.\n",
    "\n",
    "The scaled data (`scaled_price`, `scaled_rating`, `scaled_delivery_time`) can now be used for building the recommendation system, where the features are normalized and on the same scale (0-1 range), ensuring that they do not dominate the recommendation process due to differences in their original ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd56842-96dd-463e-9a53-abe5f7acc2c6",
   "metadata": {},
   "source": [
    "                       -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfcd8a-e250-42a7-8424-c3a5f8fe7a69",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90980c-d4d9-4c83-bff8-187beb67442c",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "To reduce the dimensionality of the dataset containing multiple features for predicting stock prices, you can use PCA (Principal Component Analysis) as a technique for dimensionality reduction. Here's a step-by-step explanation of how you can use PCA for this purpose:\n",
    "\n",
    "Load and preprocess the dataset: Start by loading the dataset containing the various features, such as company financial data and market trends. Perform any necessary preprocessing steps, such as handling missing values, normalizing or scaling the data, and encoding categorical variables.\n",
    "\n",
    "Separate the feature matrix: Extract the feature matrix from the dataset, which should consist of numerical features that you want to use for predicting stock prices.\n",
    "\n",
    "Standardize the features: Since PCA is sensitive to the scale of the features, it is generally a good practice to standardize the numerical features to have zero mean and unit variance. This step helps to give equal importance to all features.\n",
    "\n",
    "Apply PCA: Create an instance of the PCA class from a machine learning library like scikit-learn. Specify the desired number of components or the desired explained variance ratio to retain. Fit the PCA model to the standardized feature matrix.\n",
    "\n",
    "Analyze explained variance ratio: Check the explained variance ratio attribute of the fitted PCA model. It tells you the proportion of variance explained by each principal component. This information helps in determining the number of principal components to retain.\n",
    "\n",
    "Choose the number of components: Decide on the number of principal components to retain based on the desired explained variance ratio. You can select a threshold (e.g., 95% of variance) or choose a specific number of components that adequately capture the variability in the data.\n",
    "\n",
    "Transform the data: Use the transform method of the PCA object to project the standardized features onto the selected principal components. This step effectively reduces the dimensionality of the data.\n",
    "\n",
    "Use the reduced data: The transformed data contains the reduced set of features (principal components) that capture the most significant variability in the original data. You can now use this reduced dataset for training your stock price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a96507-1f4e-4540-88c3-66e143e0b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('stock_price_data.csv')\n",
    "features = data.drop(['target_variable'], axis=1)  # Exclude the target variable from the features\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Use the reduced features for stock price prediction\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195c9bd-aba6-49ad-a195-6b541eb54fb2",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f5010-8426-42d8-88ce-318675d1395c",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688f410-b752-449d-adc6-3a70453a125d",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "To perform Min-Max scaling on the given dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, you can follow these steps:\n",
    "\n",
    "Determine the minimum and maximum values in the dataset. In this case, the minimum value is 1, and the maximum value is 20.\n",
    "\n",
    "Apply the Min-Max scaling formula:\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "Substitute the values into the formula and calculate the scaled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1aa1bde-74fa-44a2-878f-da107b399fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "min_value = np.min(data)\n",
    "max_value = np.max(data)\n",
    "\n",
    "scaled_data = (data - min_value) / (max_value - min_value) * 2 - 1\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3b616-2f74-4a73-88b6-1f04edafdd88",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c06e381-01a3-4462-922f-7719d66e0324",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e29ed-2f91-4531-bb68-9a5781cab3f1",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "\n",
    "To perform feature extraction using PCA on the given dataset [height, weight, age, gender, blood pressure], the number of principal components to retain depends on the specific requirements of the task and the amount of variance we want to capture.\n",
    "\n",
    "Here's the general approach to determine the number of principal components to retain:\n",
    "\n",
    "Standardize the features: It is recommended to standardize the numerical features before applying PCA to ensure that features with larger scales do not dominate the principal components.\n",
    "\n",
    "Apply PCA: Create an instance of the PCA class and fit it to the standardized feature matrix.\n",
    "\n",
    "Analyze explained variance ratio: Check the explained variance ratio attribute of the fitted PCA model. This attribute tells us the proportion of variance explained by each principal component.\n",
    "\n",
    "Determine the number of components to retain: Decide on the number of principal components to retain based on the desired explained variance ratio. A common approach is to choose a threshold, such as retaining components that explain a certain percentage (e.g., 90%, 95%, etc.) of the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254d55b-8ebc-4326-8e48-47d7bc2a9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('dataset.csv')\n",
    "features = data[['height', 'weight', 'age', 'gender', 'blood pressure']]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_features)\n",
    "\n",
    "# Analyze explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Determine the number of components to retain\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "print(\"Number of components to retain:\", n_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf10c1f-245e-40f7-bc2c-e086a92eea91",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
